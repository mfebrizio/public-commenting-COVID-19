{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Commenting in a Pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Comments\n",
    "\n",
    "January through May for each year (2012 to 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "API documentation: https://regulationsgov.github.io/developers/basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# datetime package too: https://docs.python.org/3/library/datetime.html\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of the folder where the data are saved\n",
    "filePath = \"C:/Users/mark/Box Sync/_MF/Assignments/Insights/Public Commenting and COVID-19/Data/API/\"\n",
    "\n",
    "# general variables for setting parameters\n",
    "APIkey = \"fYTx9mVjuwc2ZSsdqmbgdtSqx7HGUd3aCRkiH6bC\"\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint: documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2020 - May 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/20'\n",
    "pdEnd = '05/31/20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Request URL: https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate&po=0&rpp=1000&api_key=fYTx9mVjuwc2ZSsdqmbgdtSqx7HGUd3aCRkiH6bC&pd=01%2F01%2F20-05%2F31%2F20\n",
      "\n",
      "Total number of records requested: 1368244\n",
      "Number retrieved: 1000\n",
      "\n",
      "Determine how many pages of records need to be combined via the extend method...\n",
      "Start with: 1369\n",
      "That would be enough to retrieve 1369000 records -- a margin of 756 records.\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2020Jan01_2020May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS // rpp + 1),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS // rpp + 1))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS // rpp + 1) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of data: 0\n",
      "Time array length: 1000\n",
      "nextAllowableTimeIndex = 0  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:20:25.635644\n",
      "request made (pageIndex = 0)\n",
      "Retrieved: 1000 \n",
      "\n",
      "nextAllowableTimeIndex = 100  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:28:28.349666\n",
      "request made (pageIndex = 100)\n",
      "Retrieved: 101000 \n",
      "\n",
      "nextAllowableTimeIndex = 200  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:32:11.166921\n",
      "request made (pageIndex = 200)\n",
      "Retrieved: 201000 \n",
      "\n",
      "nextAllowableTimeIndex = 300  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:35:20.872248\n",
      "request made (pageIndex = 300)\n",
      "Retrieved: 301000 \n",
      "\n",
      "nextAllowableTimeIndex = 400  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:37:50.102452\n",
      "request made (pageIndex = 400)\n",
      "Retrieved: 401000 \n",
      "\n",
      "nextAllowableTimeIndex = 500  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:40:01.720801\n",
      "request made (pageIndex = 500)\n",
      "Retrieved: 501000 \n",
      "\n",
      "nextAllowableTimeIndex = 600  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:42:33.528726\n",
      "request made (pageIndex = 600)\n",
      "Retrieved: 601000 \n",
      "\n",
      "nextAllowableTimeIndex = 700  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:45:04.967480\n",
      "request made (pageIndex = 700)\n",
      "Retrieved: 701000 \n",
      "\n",
      "nextAllowableTimeIndex = 800  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:47:34.686339\n",
      "request made (pageIndex = 800)\n",
      "Retrieved: 801000 \n",
      "\n",
      "Rate Limit remaining: 100\n",
      "nextAllowableTimeIndex = 900  nextAllowableTime = 2020-06-24 14:20:25.635644  currentTime = 2020-06-24 14:49:51.678735\n",
      "request made (pageIndex = 900)\n",
      "Retrieved: 901000 \n",
      "\n",
      "Rate Limit remaining: 75\n",
      "Rate Limit remaining: 50\n",
      "Rate Limit remaining: 25\n",
      "Rate Limit remaining: 9\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 8\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 7\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 6\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 5\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 4\n",
      "sleeping 5 minutes...\n",
      "nextAllowableTimeIndex = 0  nextAllowableTime = 2020-06-24 15:20:25.635644  currentTime = 2020-06-24 15:22:13.348450\n",
      "request made (pageIndex = 1000)\n",
      "Retrieved: 1001000 \n",
      "\n",
      "Rate Limit remaining: 9\n",
      "sleeping 5 minutes...\n",
      "Rate Limit remaining: 75\n",
      "Rate Limit remaining: 75\n",
      "Rate Limit remaining: 50\n",
      "Rate Limit remaining: 50\n",
      "nextAllowableTimeIndex = 100  nextAllowableTime = 2020-06-24 15:28:28.349666  currentTime = 2020-06-24 15:29:37.060431\n",
      "request made (pageIndex = 1100)\n",
      "Retrieved: 1101000 \n",
      "\n",
      "Rate Limit remaining: 25\n",
      "Rate Limit remaining: 25\n",
      "Rate Limit remaining: 9\n",
      "sleeping 5 minutes...\n",
      "nextAllowableTimeIndex = 200  nextAllowableTime = 2020-06-24 15:32:11.166921  currentTime = 2020-06-24 15:37:15.009956\n",
      "request made (pageIndex = 1200)\n",
      "Retrieved: 1201000 \n",
      "\n",
      "nextAllowableTimeIndex = 300  nextAllowableTime = 2020-06-24 15:35:20.872248  currentTime = 2020-06-24 15:39:44.760959\n",
      "request made (pageIndex = 1300)\n",
      "Retrieved: 1301000 \n",
      "\n",
      "If this works, we should have retrieved all the requested documents: 1368244\n",
      "Wall time: 1h 20min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported as JSON!\n"
     ]
    }
   ],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2020Jan01_2020May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agencyAcronym': 'FAA', 'allowLateComment': False, 'attachmentCount': 0, 'commentDueDate': None, 'commentStartDate': None, 'commentText': 'This nprm will effectively destroy the fpv and rc aircraft hobby along with a vibrant stem community. Strongly oppose!', 'docketId': 'FAA-2019-1100', 'docketTitle': 'Remote Identification of Unmanned Aircraft Systems ', 'docketType': 'Rulemaking', 'documentId': 'FAA-2019-1100-14848', 'documentStatus': 'Posted', 'documentType': 'Public Submission', 'numberOfCommentsReceived': 1, 'openForComment': False, 'postedDate': '2020-02-14T00:00:00-05:00', 'rin': '2120-AL31', 'submitterName': 'Chi Woodruff', 'title': 'Comment from Chi Woodruff'}\n",
      "done\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for n in range(0,len(dctsPS_all)):\n",
    "    if dctsPS_all[n]['documentId']=='FAA-2019-1100-14848':\n",
    "        print(dctsPS_all[n])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'agencyAcronym': 'DOJ', 'allowLateComment': False, 'attachmentCount': 0, 'commentDueDate': None, 'commentStartDate': None, 'commentText': 'Forced DNA collection from immigrants in detention without their consent would have wide repercussions for everyone in this country -- not only those in immigrant detention sites. In effect, this would constitute a big step toward a mass database for full population surveillance. And it would be achieved by miscasting the hundreds and thousands of children and adults in immigration detention as threats to the countrys safety. With this vast amount of sensitive information in the governments hands, the potential for abuse is too great. ', 'docketId': 'DOJ-OAG-2019-0004', 'docketTitle': 'DNA-Sample Collection From Immigration Detainees', 'docketType': 'Rulemaking', 'documentId': 'DOJ-OAG-2019-0004-18252', 'documentStatus': 'Posted', 'documentType': 'Public Submission', 'numberOfCommentsReceived': 1, 'openForComment': False, 'organization': 'American Civil Liberties Union', 'postedDate': '2020-01-01T00:00:00-05:00', 'rin': '1105-AB56', 'submitterName': 'Paulino Valenzuela', 'title': 'Comment on FR Doc # 2019-22877'}\n",
      "\n",
      "\n",
      "123456\n",
      "{'agencyAcronym': 'FTC', 'allowLateComment': False, 'attachmentCount': 0, 'commentDueDate': None, 'commentStartDate': None, 'commentText': 'You better review Coppa or YouTube will be destroyed and changed forever, YouTube Kids EXISTS FOR A REASON, USE IT!!!!!!!', 'docketId': 'FTC-2019-0054', 'docketTitle': 'FTC Seek Comments on COPPA Rule Review, 16 CFR part 312, Project No. P195404', 'docketType': 'Rulemaking', 'documentId': 'FTC-2019-0054-61638', 'documentStatus': 'Posted', 'documentType': 'Public Submission', 'numberOfCommentsReceived': 1, 'openForComment': False, 'postedDate': '2020-01-20T00:00:00-05:00', 'rin': 'Not Assigned', 'submitterName': 'Titus Przybyla', 'title': 'Comment Submitted by Titus Przybyla'}\n",
      "\n",
      "\n",
      "1234567\n",
      "{'agencyAcronym': 'CEQ', 'allowLateComment': False, 'attachmentCount': 0, 'commentDueDate': None, 'commentStartDate': None, 'commentText': \"Edward Boling,\\n\\nI write to oppose the Trump administration's proposed rollbacks that weaken the National Environmental Policy ACT (NEPA). \\n\\nThe Administration proposal threatens public health and the environment by permitting more pollution with little or no consideration of the cumulative impacts or climate change. With the climate crisis upon us, and less than ten years to make sweeping changes to our economy and society, we need every project to be thoroughly vetted for its impacts on our climate and communities. NEPA reviews must also hear from and listen to people, tribes and communities directly impacted by federal projects. The public — particularly communities most impacted by pollution and the climate crisis — must be allowed a meaningful opportunity to provide input.\\n\\nFinally, polluters must not be allowed to pick their own contractors to asses environmental impacts. These should be done by federal agencies or third-party agents with no potential or apparent conflict of interest.\\n\\nAny changes that restrict public input, limit consideration of project alternatives, or narrow or eliminate federal agencies’ obligations to consider a project’s climate impacts are unacceptable. I oppose the Administration's proposed changes to NEPA in the strongest possible terms. \\n\\nMich Kraft \\nMkg1863@gmail.com \\n703 bridge rd \\nAkron, Ohio 44312\\n\\n\\n \\n\", 'docketId': 'CEQ-2019-0003', 'docketTitle': 'Update to the Regulations Implementing the Procedural Provisions of the National Environmental Policy Act', 'docketType': 'Rulemaking', 'documentId': 'CEQ-2019-0003-596144', 'documentStatus': 'Posted', 'documentType': 'Public Submission', 'numberOfCommentsReceived': 1, 'openForComment': False, 'postedDate': '2020-05-23T00:00:00-04:00', 'rin': '0331-AA03', 'submitterName': 'Mich Kraft', 'title': 'MM85 Comment Submitted by Mich Kraft'}\n",
      "\n",
      "\n",
      "last\n",
      "{'agencyAcronym': 'FSA', 'allowLateComment': False, 'attachmentCount': 0, 'commentDueDate': None, 'commentStartDate': None, 'commentText': 'We are a young and small vineyard with 3,800 wine grape vines on about 7 acres of our 15.95 acre parcel. We grow these grapes to sell to wineries which we have done for the past four years. I have been informed that one winery to which I have sold wine grapes for three years that it will not be buying grapes this year. The reason given is that it was forced to close by the pandemic and is not open yet. I was informed that its inventory of finished wines has not shrunk by sales due to the forced closure. I also planted an acre of Grenache Noir vines on the prospect of selling them to one winery, as it had promised to do, but it will buy less Sangiovese and no Grenache for the same reason as the first winery I mentioned. Another winery which had promised to buy Tempranillo advised me it would not be buying grapes. I have had no trouble in the past in selling all the grapes we produce. This is an inopportune time for us as our volume is growing with the age of the vines. I expect to lose about 60% of our production, equal to approximately $9,600. ', 'docketId': 'FSA-2020-0004', 'docketTitle': 'Coronavirus Food Assistance Program', 'docketType': 'Rulemaking', 'documentId': 'FSA-2020-0004-0283', 'documentStatus': 'Posted', 'documentType': 'Public Submission', 'numberOfCommentsReceived': 1, 'openForComment': False, 'postedDate': '2020-05-31T00:00:00-04:00', 'rin': '0503-AA65', 'submitterName': 'William Rice', 'title': 'Comment on FR Doc # 2020-11155'}\n"
     ]
    }
   ],
   "source": [
    "print('0',dctsPS_all[0],'\\n',\n",
    "      '123456',dctsPS_all[123456],'\\n',\n",
    "      '1234567',dctsPS_all[1234567],'\\n',\n",
    "      'last',dctsPS_all[-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1368244 entries, 0 to 1368243\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count    Dtype \n",
      "---  ------                    --------------    ----- \n",
      " 0   agencyAcronym             1368244 non-null  object\n",
      " 1   allowLateComment          1368244 non-null  bool  \n",
      " 2   attachmentCount           1368244 non-null  int64 \n",
      " 3   commentDueDate            0 non-null        object\n",
      " 4   commentStartDate          0 non-null        object\n",
      " 5   commentText               1368150 non-null  object\n",
      " 6   docketId                  1368244 non-null  object\n",
      " 7   docketTitle               1368244 non-null  object\n",
      " 8   docketType                1368244 non-null  object\n",
      " 9   documentId                1368244 non-null  object\n",
      " 10  documentStatus            1368244 non-null  object\n",
      " 11  documentType              1368244 non-null  object\n",
      " 12  numberOfCommentsReceived  1368244 non-null  int64 \n",
      " 13  openForComment            1368244 non-null  bool  \n",
      " 14  organization              84947 non-null    object\n",
      " 15  postedDate                1368244 non-null  object\n",
      " 16  rin                       1225512 non-null  object\n",
      " 17  submitterName             1269580 non-null  object\n",
      " 18  title                     1368244 non-null  object\n",
      "dtypes: bool(2), int64(2), object(15)\n",
      "memory usage: 180.1+ MB\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert to pandas DataFrame\n",
    "df2020 = pd.DataFrame(dctsPS_all)\n",
    "df2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agencyAcronym', 'allowLateComment', 'attachmentCount', 'commentDueDate', 'commentStartDate', 'commentText', 'docketId', 'docketTitle', 'docketType', 'documentId', 'documentStatus', 'documentType', 'numberOfCommentsReceived', 'openForComment', 'organization', 'postedDate', 'rin', 'submitterName', 'title']\n"
     ]
    }
   ],
   "source": [
    "dfColumns = df2020.columns.tolist()\n",
    "print(dfColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_columns = ['agencyAcronym','attachmentCount','commentDueDate','commentStartDate','docketId',\n",
    "                 'docketType','documentId','numberOfCommentsReceived','openForComment','postedDate',\n",
    "                 'submitterName','title','organization']\n",
    "\n",
    "savePath = 'C:/Users/mark/Box Sync/_MF/Assignments/Insights/Public Commenting and COVID-19/Data/Annual/'\n",
    "saveFile = 'endpoint_documents_PS_2020.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    df2020.to_csv(outfile, index_label='index', line_terminator='\\n', columns=write_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2019 - May 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/19'\n",
    "pdEnd = '05/31/19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2019Jan01_2019May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2019Jan01_2019May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2018 - May 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/18'\n",
    "pdEnd = '05/31/18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2018Jan01_2018May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2018Jan01_2018May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2017 - May 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/17'\n",
    "pdEnd = '05/31/17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2017Jan01_2017May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2017Jan01_2017May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2016 - May 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/16'\n",
    "pdEnd = '05/31/16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2016Jan01_2016May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2016Jan01_2016May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2015 - May 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/15'\n",
    "pdEnd = '05/31/15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2015Jan01_2015May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2015Jan01_2015May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2014 - May 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/14'\n",
    "pdEnd = '05/31/14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2014Jan01_2014May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2014Jan01_2014May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2013 - May 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/13'\n",
    "pdEnd = '05/31/13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2013Jan01_2013May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2013Jan01_2013May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Comments: Jan 2012 - May 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset parameters\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "\n",
    "# variables for setting parameters: document type = Public Submission\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dct=PS&so=ASC&sb=postedDate\"\n",
    "pdStart = '01/01/12'\n",
    "pdEnd = '05/31/12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'pd': pdStart+'-'+pdEnd}\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2012Jan01_2012May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS//rpp + (1 if (numPS%rpp>0) else 0)),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS//rpp + (1 if (numPS%rpp>0) else 0)) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2012Jan01_2012May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
