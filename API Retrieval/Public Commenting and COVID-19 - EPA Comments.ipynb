{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# datetime package too: https://docs.python.org/3/library/datetime.html\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of the folder where the data are saved\n",
    "filePath = \"DESIGNATE_FILE_PATH\"\n",
    "\n",
    "# general variables for setting parameters\n",
    "APIkey = \"INSERT_API_KEY\"\n",
    "rpp = 1000\n",
    "pageIndex = 0\n",
    "po = pageIndex * rpp\n",
    "agency = 'EPA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Submissions: Jan 2020 - June 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for setting parameters\n",
    "# document type = Public Submission\n",
    "# Rulemaking dockets only\n",
    "# range: receivedDate\n",
    "\n",
    "baseURL_PS = \"https://api.data.gov/regulations/v3/documents.json?encoded=1&countsOnly=0&dkt=R&so=ASC&sb=postedDate\"\n",
    "dctType = 'PS'\n",
    "dateRangeStart = '01/01/20'\n",
    "dateRangeEnd = '06/30/20'\n",
    "\n",
    "# set parameters to retrieve PS documents\n",
    "params = {'po': po,\n",
    "          'rpp': rpp,\n",
    "          'api_key': APIkey,\n",
    "          'rd': dateRangeStart+'-'+dateRangeEnd, \n",
    "          'dct': dctType, \n",
    "          'a': agency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- RETRIEVE COMMENTS ----- #\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "dcts_response = requests.get(baseURL_PS, params=params)\n",
    "RL_remaining = int(dcts_response.headers['X-RateLimit-Remaining'])\n",
    "print(\"Status Code: \"+str(dcts_response.status_code),\n",
    "      'Request URL: '+str(dcts_response.request.url)+'\\n',sep='\\n')\n",
    "\n",
    "# nested list: separate 'documents' from 'totalNumRecords'\n",
    "# confirm total requested and number of documents retrieved\n",
    "numPS = dcts_response.json()['totalNumRecords']\n",
    "dctsPS = dcts_response.json()['documents']\n",
    "print('Total number of records requested: '+str(numPS), 'Number retrieved: '+str(len(dctsPS)), sep='\\n')\n",
    "\n",
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS)==numPS:\n",
    "    with open(filePath+'endpoint_documents_PS_2020Jan01_2020May31.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS, outfile, ensure_ascii=False, indent=4)    \n",
    "    print('Exported as JSON!')\n",
    "        \n",
    "else:\n",
    "    print('\\n''Determine how many pages of records need to be combined via the extend method...',\n",
    "          'Start with: '+str(numPS // rpp + 1),\n",
    "          'That would be enough to retrieve '+str(rpp * (numPS // rpp + 1))+' records'\n",
    "          ' -- a margin of '+str(rpp * (numPS // rpp + 1) - numPS)+' records.',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define empty object to put extended data\n",
    "dctsPS_all = []\n",
    "totalNumPages = numPS//rpp + (1 if (numPS%rpp>0) else 0)\n",
    "print('Initial length of data: '+str(len(dctsPS_all)))\n",
    "\n",
    "# define time objects for avoiding rate limit\n",
    "initialNextTime = datetime.now()\n",
    "nextAllowableTime = []\n",
    "pagesPerHour = 1000 ## regulations.gov rate limit of 1000\n",
    "\n",
    "# fill array of allowable times\n",
    "for index in range(0,pagesPerHour):\n",
    "    nextAllowableTime.append(initialNextTime)\n",
    "print('Time array length: '+str(len(nextAllowableTime)))\n",
    "\n",
    "# retrieve additional pages of documents and extend object\n",
    "for pageIndex in range (0,totalNumPages): ## remember range is non-inclusive\n",
    "    \n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 5 minutes...\", sep='\\n')\n",
    "        time.sleep(300)\n",
    "    elif (RL_remaining <= 100) & (RL_remaining%25==0):\n",
    "        print('Rate Limit remaining: '+str(RL_remaining))\n",
    "    \n",
    "    nextAllowableTimeIndex = pageIndex % pagesPerHour\n",
    "    currentTime = datetime.now()\n",
    "    if pageIndex%100 == 0:\n",
    "        print(\"nextAllowableTimeIndex = \"+str(nextAllowableTimeIndex),\n",
    "              \"nextAllowableTime = \"+str(nextAllowableTime[nextAllowableTimeIndex]),\n",
    "              \"currentTime = \"+str(currentTime), sep=\"  \")\n",
    "\n",
    "    if currentTime < nextAllowableTime[nextAllowableTimeIndex]:\n",
    "        waitTime = nextAllowableTime[nextAllowableTimeIndex] - currentTime\n",
    "        print(\"sleeping \" + str(waitTime.total_seconds()) + \" seconds...\")\n",
    "        time.sleep(waitTime.total_seconds() + 0.01)\n",
    "    \n",
    "    if nextAllowableTime[nextAllowableTimeIndex] <= datetime.now():\n",
    "        nextAllowableTime[nextAllowableTimeIndex] = datetime.now() + timedelta(seconds = 3600) ## add one hour to nextAllowableTime\n",
    "\n",
    "        try:\n",
    "            po = pageIndex * rpp\n",
    "            params.update({'po': po})\n",
    "            temp_response = requests.get(baseURL_PS, params=params)\n",
    "            RL_remaining = int(temp_response.headers['X-RateLimit-Remaining'])\n",
    "            if temp_response.status_code != 200: ## status code = 429 means over rate limit\n",
    "                print('code '+str(temp_response.status_code)+' for page #'+str(pageIndex),\n",
    "                      temp_response.text, sep='\\n')\n",
    "\n",
    "            data_this_page = temp_response.json()['documents']\n",
    "            dctsPS_all.extend(data_this_page)\n",
    "            if pageIndex%100 == 0:\n",
    "                print(\"request made (pageIndex = \" + str(pageIndex) + \")\")\n",
    "                print('Retrieved: '+str(len(dctsPS_all)),'\\n')\n",
    "        except:\n",
    "            print('missing page: '+str(pageIndex))\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        print(\"request failed\")\n",
    "        print(\"too soon -- breaking (pageIndex = \"+str(pageIndex)+\")\")\n",
    "        break\n",
    "\n",
    "print('If this works, we should have retrieved all the requested documents: '+str(len(dctsPS_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if requested == retrieved, then export as JSON\n",
    "if len(dctsPS_all)==numPS:\n",
    "    dataFile = 'EPA_endpoint_documents_PS_2020Jan_2020Jun.json'\n",
    "    with open(filePath+dataFile, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(dctsPS_all, outfile, ensure_ascii=False, indent=4)\n",
    "    print('Exported as JSON!')\n",
    "else:\n",
    "    print('Export unsuccessful. Check your code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('0',dctsPS_all[0],'\\n',\n",
    "      'last',dctsPS_all[-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas DataFrame\n",
    "df2020PS = pd.DataFrame(dctsPS_all)\n",
    "df2020PS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with document URL \n",
    "df2020PS['documentURL'] = \"https://www.regulations.gov/document?D=\"\n",
    "df2020PS.loc[:,'documentURL'] = df2020PS['documentURL']+df2020PS['documentId']\n",
    "print(df2020PS.loc[0,'documentURL'], \n",
    "      df2020PS.loc[1,'documentURL'], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print columns list -- determine which ones to write to CSV\n",
    "dfColumns = df2020PS.columns.tolist()\n",
    "print(dfColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_columns = ['agencyAcronym','docketId','docketType','rin',\n",
    "                 'documentId','documentType','numberOfCommentsReceived','postedDate',\n",
    "                 'title','commentText','attachmentCount','documentURL']\n",
    "\n",
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'EPA_endpoint_documents_PS_2020.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    df2020PS.to_csv(outfile, index_label='index', line_terminator='\\n', columns=write_columns)\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Submissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV of Public Submissions data\n",
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "fileName = 'EPA_endpoint_documents_PS_2020.csv'\n",
    "\n",
    "with open(savePath+fileName,'r',encoding='utf-8') as loadfile:\n",
    "    dfPS = pd.read_csv(loadfile, index_col='index')\n",
    "dfPS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for commentsPosted (==1 for each obs)\n",
    "dfPS['commentsPosted'] = 1\n",
    "\n",
    "# rename column\n",
    "dfPS = dfPS.rename(columns={'numberOfCommentsReceived': 'commentsReceived'})\n",
    "\n",
    "# create strings that will convert to naive datetime\n",
    "dfPS['postedDateNaive'] = dfPS['postedDate'].str.slice(start=0,stop=10)\n",
    "\n",
    "# convert to datetime format\n",
    "dfPS['dtPosted'] = pd.to_datetime(dfPS['postedDateNaive'], infer_datetime_format=True)\n",
    "\n",
    "# generate year and month columns\n",
    "dfPS['postedMonth'] = dfPS['dtPosted'].dt.month\n",
    "dfPS['postedYear'] = dfPS['dtPosted'].dt.year\n",
    "dfPS.loc[:,['dtPosted','postedMonth','postedYear','commentsPosted','commentsReceived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get receivedDate for Top 30 Dockets\n",
    "from: Export Docket Folder (export all as csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top30Received_by_Docket = pd.pivot_table(dfPS, values=['commentsPosted','commentsReceived','postedMonth'], index=['docketId'], \n",
    "               aggfunc={'commentsPosted': np.sum,\n",
    "                        'commentsReceived': np.sum,\n",
    "                        'postedMonth': np.max}\n",
    "              ).sort_values('commentsReceived', ascending=False).head(30)\n",
    "Top30Received_by_Docket['docketURL'] = 'https://www.regulations.gov/docket?D='+Top30Received_by_Docket.index\n",
    "Top30Received_by_Docket['exportURL'] = 'https://www.regulations.gov/exportdocket?docketId='+Top30Received_by_Docket.index\n",
    "top30DktList = Top30Received_by_Docket.index.tolist()\n",
    "rdSample = Top30Received_by_Docket.sum(0)['commentsPosted']\n",
    "print(rdSample)\n",
    "Top30Received_by_Docket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(top30DktList)):\n",
    "    print('Docket '+str(n)+': '+Top30Received_by_Docket['exportURL'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set variables outside the for loop\n",
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "docId = []\n",
    "docType = []\n",
    "docSub = []\n",
    "rDate = []\n",
    "pmDate = []\n",
    "\n",
    "for dktId in range(len(top30DktList)):\n",
    "    # update dktFile pointer\n",
    "    dktFile = 'DOCKET_'+str(top30DktList[dktId])+'.csv'\n",
    "\n",
    "    # load csv\n",
    "    with open(savePath+dktFile,'r', encoding='utf-8') as loadfile:\n",
    "        dfTopDkt = pd.read_csv(loadfile, skiprows=list(range(0,5)), \n",
    "                               usecols=['Document ID','Document Type','Document SubType','Received Date','Post Mark Date'],\n",
    "                               dtype={'Document ID': 'str', 'Document Type': 'str', 'Document SubType': 'str'})\n",
    "    \n",
    "    # print length of documents for dktId\n",
    "    print(str(top30DktList[dktId])+': '+str(len(dfTopDkt)))\n",
    "    \n",
    "    # narrow DataFrame and fix column names\n",
    "    dfTopDkt = dfTopDkt.rename(columns={'Document ID': 'documentId', \n",
    "                                        'Document Type': 'documentType', \n",
    "                                        'Document SubType': 'documentSubType', \n",
    "                                        'Received Date': 'receivedDate', \n",
    "                                        'Post Mark Date': 'postmarkDate'})\n",
    "\n",
    "    docId.extend(dfTopDkt['documentId'].tolist())\n",
    "    docType.extend(dfTopDkt['documentType'].tolist())\n",
    "    docSub.extend(dfTopDkt['documentSubType'].tolist())\n",
    "    rDate.extend(dfTopDkt['receivedDate'].tolist())\n",
    "    pmDate.extend(dfTopDkt['postmarkDate'].tolist())\n",
    "    \n",
    "    # print length of longest list\n",
    "    print(max([len(docId), len(docType), len(docSub), len(rDate), len(pmDate)]),'\\n')\n",
    "    \n",
    "dfTopDktcombo = pd.DataFrame(zip(docId, docType, docSub, rDate, pmDate), \n",
    "                             columns=['documentId','documentType','documentSubType','receivedDate','postmarkDate'])\n",
    "\n",
    "# remove obs missing documentId (e.g., \"withdrawn\" documents)\n",
    "print(len(dfTopDktcombo))\n",
    "dfTopDktcombo = dfTopDktcombo[dfTopDktcombo['documentId'].notna()]\n",
    "print(len(dfTopDktcombo))\n",
    "\n",
    "# merge dataframes on documentId\n",
    "print(rdSample,'\\n') ## compare length to sample of top 20 dockets\n",
    "dfPSrd = dfPS.merge(dfTopDktcombo, how='left', on=['documentId'], indicator=True, validate=\"1:1\")\n",
    "print(dfPSrd['_merge'].value_counts(),'\\n')\n",
    "dfPSrd = dfPSrd.rename(columns={'_merge': '_mergeTop30'}) ## rename _merge column\n",
    "dfPSrd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfPSrd['documentType_x'].value_counts(),\n",
    "      dfPSrd['documentType_y'].value_counts() ,sep='\\n')\n",
    "\n",
    "dfPSrd = dfPSrd.drop(columns=['documentType_y'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get receivedDate for remaining comments\n",
    "from: API document endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of comments missing receivedDate by month\n",
    "dfPSrd.loc[dfPSrd['receivedDate'].isna(),\n",
    "           ['documentId','postedDate','postedMonth']].groupby('postedMonth').documentId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of docIds for API request\n",
    "bool_missingRD = dfPSrd['receivedDate'].isna()\n",
    "\n",
    "missingRD = dfPSrd.loc[bool_missingRD,'documentId'].tolist()\n",
    "\n",
    "print(bool_missingRD.value_counts(),\n",
    "      len(missingRD), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Create new DataFrame for Remaining Comments ----- #\n",
    "\n",
    "# ----- Retrieve receivedDates for comments ----- #\n",
    "import requests\n",
    "\n",
    "# general variables for setting parameters\n",
    "APIkey = \"INSERT_API_KEY\"\n",
    "baseURL = \"https://api.data.gov:443/regulations/v3/document.json?\"\n",
    "dctId = \"\"\n",
    "\n",
    "# set parameters\n",
    "params = {'api_key': APIkey,\n",
    "          'documentId': dctId}\n",
    "\n",
    "rangeRD = len(missingRD)\n",
    "listRD = [] # list for adding receivedDate of each entry\n",
    "\n",
    "# retrieve comments using Requests library and check GET request response \n",
    "for d in range(rangeRD):\n",
    "    dctId = missingRD[d]\n",
    "    params.update({'documentId': dctId})\n",
    "\n",
    "    dct_response = requests.get(baseURL, params=params)\n",
    "    RL_remaining = int(dct_response.headers['X-RateLimit-Remaining'])\n",
    "\n",
    "    if dct_response.status_code != 200:\n",
    "        print('code '+str(dct_response.status_code)+' for page #'+str(pageIndex), \n",
    "              dct_response.text, sep='\\n')\n",
    "    if RL_remaining < 10:\n",
    "        print('Rate Limit remaining: '+str(RL_remaining),\n",
    "              \"sleeping 1 minute...\", sep='\\n')\n",
    "        time.sleep(60)\n",
    "\n",
    "    this_receivedDate = dct_response.json()['receivedDate']\n",
    "    listRD.append(this_receivedDate)\n",
    "    \n",
    "    if d%100==0:\n",
    "        print(\"Number of comments retrieved: \"+str(d))\n",
    "\n",
    "print('Length of receivedDate list is '+str(len(listRD)))\n",
    "\n",
    "# ----- Generate df from the lists ----- #\n",
    "remainingList = list(zip(missingRD, listRD))\n",
    "dfRemaining = pd.DataFrame(remainingList, columns = ['documentId', 'receivedDate'])\n",
    "dfRemaining.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate and Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dfs so we have one df with receivedDate\n",
    "dfRD = pd.concat([dfTopDktcombo, dfRemaining], \n",
    "                 axis=0, join='outer', ignore_index=True, verify_integrity=True)\n",
    "\n",
    "# merge dataframes on documentId\n",
    "dfPS2020 = dfPS.merge(dfRD, how='left', on=['documentId'], indicator=True, validate=\"1:1\")\n",
    "print(dfPS2020['_merge'].value_counts(),'\\n')\n",
    "dfPS2020 = dfPS2020.rename(columns={'_merge': '_mergeRD'})\n",
    "dfPS2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime format\n",
    "dfPS2020['receivedDateNaive'] = dfPS2020['receivedDate'].str.slice(start=0,stop=10)\n",
    "dfPS2020['dtReceived'] = pd.to_datetime(dfPS2020['receivedDateNaive'])\n",
    "\n",
    "dfPS2020['receivedMonth'] = dfPS2020['dtReceived'].dt.month\n",
    "dfPS2020.loc[:,['receivedDate','dtReceived','receivedMonth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(dfPS2020, values=['commentsPosted'], index=['receivedMonth'], columns=['postedMonth'], \n",
    "               aggfunc={'commentsPosted': np.sum}, fill_value=0, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Select Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for dropping PS documents\n",
    "dropPS = []\n",
    "\n",
    "### remove documents posted before 2020\n",
    "dropPS = dfPS2020[dfPS2020['dtPosted']<datetime(2020,1,1)].index.tolist()\n",
    "print(len(dropPS),'\\n')\n",
    "\n",
    "### remove documents posted in July 2020\n",
    "dropPS.extend(dfPS2020[dfPS2020['dtReceived']>=datetime(2020,7,1)].index.tolist())\n",
    "print(len(dropPS),'\\n')\n",
    "\n",
    "# drop entries\n",
    "print(len(dfPS2020))\n",
    "dfPS2020 = dfPS2020.drop(index=dropPS, errors='ignore') ## ignore → only existing labels are dropped\n",
    "print(len(dfPS2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: Clean Public Submissions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(dfPS2020, values=['commentsPosted','commentsReceived'], index=['receivedMonth'],\n",
    "               aggfunc=np.sum, fill_value=0, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15411158/pandas-countdistinct-equivalent\n",
    "print(dfPS2020.groupby('postedMonth').docketId.nunique(),\n",
    "      dfPS2020.groupby('receivedMonth').docketId.nunique(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'EPA_cleaned_PS_2020.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    dfPS2020.to_csv(outfile, index_label='index', line_terminator='\\n')\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load CSV of Public Submissions data\n",
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "fileName = 'EPA_cleaned_PS_2020.csv'\n",
    "\n",
    "with open(savePath+fileName,'r',encoding='utf-8') as loadfile:\n",
    "    dfPS2020 = pd.read_csv(loadfile, index_col='index')\n",
    "dfPS2020.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter for Science Rule\n",
    "bool_science = dfPS2020['docketId']=='EPA-HQ-OA-2018-0259'\n",
    "dfPS2020.loc[bool_science,'scienceRule'] = 1\n",
    "dfPS2020.loc[~bool_science,'scienceRule'] = 0\n",
    "dfPS2020['scienceRule'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Submissions per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfPS2020.groupby(['receivedMonth','docketId'])))\n",
    "print(sum(dfPS2020.groupby('receivedMonth').docketId.nunique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include all dockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame with sum of comments per month\n",
    "dfCommentsMonthly = dfPS2020.groupby('receivedMonth')[['commentsPosted','commentsReceived']].sum().reset_index()\n",
    "\n",
    "# add new column with unique dockets receiving comments per month\n",
    "dfCommentsMonthly['docketsUnique'] = dfPS2020.groupby('receivedMonth')['docketId'].nunique().tolist()\n",
    "\n",
    "# add column for month labels\n",
    "dfCommentsMonthly.insert(1,'labelMonth',['Jan','Feb','Mar','Apr','May','Jun'])\n",
    "\n",
    "# rename columns\n",
    "dfCommentsMonthly = dfCommentsMonthly.rename(columns={'commentsPosted': 'commentsUnique', 'commentsReceived': 'commentsAll'})\n",
    "\n",
    "# calculate two new columns: comments per unique dockets\n",
    "dfCommentsMonthly['unq_per_dkts'] = dfCommentsMonthly['commentsUnique']/dfCommentsMonthly['docketsUnique']\n",
    "dfCommentsMonthly['all_per_dkts'] = dfCommentsMonthly['commentsAll']/dfCommentsMonthly['docketsUnique']\n",
    "\n",
    "# view returned df\n",
    "dfCommentsMonthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'data_for_analysis_monthly.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    dfCommentsMonthly.to_csv(outfile, line_terminator='\\n', index=False)\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Science rule filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame with sum of comments per month\n",
    "dfCommentsMonthly_filterSR = dfPS2020.groupby(['scienceRule','receivedMonth'])[['commentsPosted','commentsReceived']].sum().reset_index()\n",
    "\n",
    "# add new column with unique dockets receiving comments per month\n",
    "dfCommentsMonthly_filterSR['docketsUnique'] = dfPS2020.groupby(['scienceRule','receivedMonth'])['docketId'].nunique().tolist()\n",
    "\n",
    "# add column for month labels\n",
    "dfCommentsMonthly_filterSR.insert(2,'labelMonth',['Jan','Feb','Mar','Apr','May','Jun','Jan','Mar','Apr','May','Jun'])\n",
    "\n",
    "# rename columns\n",
    "dfCommentsMonthly_filterSR = dfCommentsMonthly_filterSR.rename(columns={'commentsPosted': 'commentsUnique', 'commentsReceived': 'commentsAll'})\n",
    "\n",
    "# calculate two new columns: comments per unique dockets\n",
    "exclude_science_rule = dfCommentsMonthly_filterSR['scienceRule']==0\n",
    "dfCommentsMonthly_filterSR.loc[exclude_science_rule,'unq_per_dkts'] = dfCommentsMonthly_filterSR['commentsUnique']/dfCommentsMonthly_filterSR['docketsUnique']\n",
    "dfCommentsMonthly_filterSR.loc[exclude_science_rule,'all_per_dkts'] = dfCommentsMonthly_filterSR['commentsAll']/dfCommentsMonthly_filterSR['docketsUnique']\n",
    "\n",
    "# view returned df\n",
    "dfCommentsMonthly_filterSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'data_for_analysis_monthly_filterSR.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    dfCommentsMonthly_filterSR.to_csv(outfile, line_terminator='\\n', index=False)\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Submissions per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfPS2020.groupby('dtReceived')[['commentsPosted','commentsReceived']]))\n",
    "print(len(dfPS2020.groupby('dtReceived')['docketId'].nunique().tolist()))\n",
    "\n",
    "print(len(dfPS2020.groupby(['scienceRule','dtReceived'])[['commentsPosted','commentsReceived']]))\n",
    "print(len(dfPS2020.groupby(['scienceRule','dtReceived'])['docketId'].nunique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include all dockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame with sum of comments per month\n",
    "dfCommentsDaily = dfPS2020.groupby('dtReceived')[['commentsPosted','commentsReceived']].agg(np.sum).reset_index()\n",
    "\n",
    "# add new column with unique dockets receiving comments per month\n",
    "dfCommentsDaily['docketsUnique'] = dfPS2020.groupby('dtReceived')['docketId'].nunique().tolist()\n",
    "\n",
    "# rename columns\n",
    "dfCommentsDaily = dfCommentsDaily.rename(columns={'commentsPosted': 'commentsUnique', 'commentsReceived': 'commentsAll'})\n",
    "\n",
    "# calculate two new columns: comments per unique dockets\n",
    "dfCommentsDaily['unq_per_dkts'] = dfCommentsDaily['commentsUnique']/dfCommentsDaily['docketsUnique']\n",
    "dfCommentsDaily['all_per_dkts'] = dfCommentsDaily['commentsAll']/dfCommentsDaily['docketsUnique']\n",
    "\n",
    "# view returned df\n",
    "dfCommentsDaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'data_for_analysis_daily.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    dfCommentsDaily.to_csv(outfile, line_terminator='\\n', index=False)\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Science rule filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DataFrame with sum of comments per month\n",
    "dfCommentsDaily_filterSR = dfPS2020.groupby(['scienceRule','dtReceived'])[['commentsPosted','commentsReceived']].agg(np.sum).reset_index()\n",
    "\n",
    "# add new column with unique dockets receiving comments per month\n",
    "dfCommentsDaily_filterSR['docketsUnique'] = dfPS2020.groupby(['scienceRule','dtReceived'])['docketId'].nunique().tolist()\n",
    "\n",
    "# rename columns\n",
    "dfCommentsDaily_filterSR = dfCommentsDaily_filterSR.rename(columns={'commentsPosted': 'commentsUnique', 'commentsReceived': 'commentsAll'})\n",
    "\n",
    "# calculate two new columns: comments per unique dockets\n",
    "exclude_science_rule = dfCommentsDaily_filterSR['scienceRule']==0\n",
    "dfCommentsDaily_filterSR.loc[exclude_science_rule,'unq_per_dkts'] = dfCommentsDaily_filterSR['commentsUnique']/dfCommentsDaily_filterSR['docketsUnique']\n",
    "dfCommentsDaily_filterSR.loc[exclude_science_rule,'all_per_dkts'] = dfCommentsDaily_filterSR['commentsAll']/dfCommentsDaily_filterSR['docketsUnique']\n",
    "\n",
    "# view returned df\n",
    "dfCommentsDaily_filterSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "saveFile = 'data_for_analysis_daily_filterSR.csv'\n",
    "\n",
    "# write to csv, reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\n",
    "with open(savePath+saveFile, 'w', encoding='utf-8') as outfile:\n",
    "    dfCommentsDaily_filterSR.to_csv(outfile, line_terminator='\\n', index=False)\n",
    "\n",
    "print('Saved as CSV!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load CSV of Public Submissions data\n",
    "savePath = 'DESIGNATE_FILE_PATH'\n",
    "fileName = 'EPA_cleaned_PS_2020.csv'\n",
    "\n",
    "with open(savePath+fileName,'r',encoding='utf-8') as loadfile:\n",
    "    dfPS2020 = pd.read_csv(loadfile, index_col='index')\n",
    "dfPS2020.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Dockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view top 5 dockets in terms of total comments\n",
    "dfTop5Dkts = pd.pivot_table(dfPS2020, values=['commentsPosted','commentsReceived'], index=['docketId'], \n",
    "                            aggfunc=np.sum, fill_value=0).sort_values('commentsReceived', ascending=False).head(5)\n",
    "dfTop5Dkts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view commentsReceived per month on top 5 dockets\n",
    "select_dockets = dfTop5Dkts.index.tolist()\n",
    "\n",
    "bool_select = [True if doc in select_dockets else False for doc in dfPS2020['docketId'].tolist()]\n",
    "print(bool_select.count(True))\n",
    "\n",
    "pd.pivot_table(dfPS2020[bool_select], values=['commentsPosted','commentsReceived'], index=['docketId','receivedMonth'],\n",
    "               aggfunc=np.sum, fill_value=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
